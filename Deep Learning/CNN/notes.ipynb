{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b3e6432",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN): \n",
    "##### It is a powerful type of artificial neural network, primarily used for image recognition and processing.\n",
    "##### We directly pass the image to the network without flattening it first. A CNN is designed to work with the grid-like structure of an image.\n",
    "##### A grayscale image is a 2D array of pixel values (height x width).\n",
    "##### A color image is typically a 3D array (height x width x color channels(RGB)).\n",
    "##### In this we update filters values, unlike in ANN we update weight and bias.\n",
    "\n",
    "#### CNN Architecture:\n",
    "##### 1- Input Layer: Receives the raw pixel values of the image in its original 2D or 3D format.\n",
    "##### 2- Convolutional Layers: A convolutional layer consists of multiple filters that slide over the input image to create feature maps. After the convolutioncan activation function like ReLU (Rectified Linear Unit) is usually applied.\n",
    "##### 3- Pooling Layers: These layers are often placed after convolutional layers to down-sample the feature maps, reducing their size and computational complexity.\n",
    "##### 4- Fully Connected Layers: ATthe high-level features are flattened into a one-dimensional vector. This vector is then fed into one or more fully connected layers, which are standard neural network layers.\n",
    "##### 5- Output Layer:\n",
    "\n",
    "##### Filter:\n",
    "##### We decide the size of filter, mostly (3 x 3) filter size is used. And then our filter is filled with values when we apply that filter on image.\n",
    "##### Efficiency: Smaller filters are computationally cheaper.\n",
    "##### Feature Representation: Stacking multiple layers of small filters can learn more complex features than a single layer of large filters.\n",
    "##### Deeper Networks: Using smaller filters allows for deeper, more powerful networks.\n",
    "##### To check the size of image after applying filter then, [(n - f) + 1] where n is size of original image and f is size of filter.\n",
    "\n",
    "##### Stride:\n",
    "##### It means the jump of a filter. A stride of 1 means the filter slides one pixel at a time. A stride of 2 means the filter moves two pixels at a time.\n",
    "##### Using a larger stride can reduce the computational cost and the size of the output feature map.\n",
    "\n",
    "##### Pooling: \n",
    "##### Pooling is a down-sampling technique used to reduce the spatial dimensions (width and height) of the feature maps. It helps to make the network more computationally efficient and also makes the learned features more robust to small translations in the input image.\n",
    "##### The most common pooling filter size is 2x2, usually combined with a stride of 2.\n",
    "##### Types of pooling:\n",
    "##### 1- Max Pooling. 2- Average Pooling.\n",
    "\n",
    "\n",
    "##### Issues and Solution:\n",
    "##### Vanishing and Exploiding Gradient: Weight initialize, Activation Function, Batch Normalization(training fast and stable, mean=0, std=1), Gradient Clipping(exploding gradients in RNN).\n",
    "##### Not enought data: Transfer learning, Un-supervised pre-training.\n",
    "##### Slow training: Optimizers, learning rate.\n",
    "##### Over Training: L1 and l2 regularization, Dropout.\n",
    "\n",
    "##### Early Stopping: A technique which stop the training if there is no changes during some epochs.\n",
    "\n",
    "#### Dropout:\n",
    "##### When one neuron is learning is more and other don't then it leads the model to overfitting. To prevent this we apply dropout.\n",
    "##### In this we randomly off neurons. If neuron 1 is off in first epoch then it will on in second epoch.\n",
    "##### It only works during training.\n",
    "##### It increased the accuracy by 2%.\n",
    "##### During testing, we don't give the whole weights to neurons, instead of this we give the probability to that neuron during the training it had.\n",
    "\n",
    "##### Drawbacks: Trainig becomes slow.\n",
    "\n",
    "#### Regularization:\n",
    "##### When we have more neurons in one hidden layer, the capabilty to capture different pattern is equal to the number of neurons which again may leads to overfitting.\n",
    "##### To solve overfitting we increase our dataset.\n",
    "\n",
    "#### Activation Functions:\n",
    "##### Aactivation functions are essential for introducing non-linearity into neural networks, enabling them to learn and model complex, real-world data. \n",
    "##### ReLU: Characteristics ==> Non-linear, Computational inexpensive, Non-saturated(can't sequeeze data), Converge(final step) faster than sigmoid and tanh. f(x) = max(0, x).\n",
    "##### Disadvantage:\n",
    "##### It's not zero centroid and not completely differentiable.\n",
    "\n",
    "##### Dying Relu Problem:\n",
    "##### If 50% of neurons are dead then no patterns captured.\n",
    "##### Reason of dead is, when our function [z1 = w1x1 + w2x2 + b < 0] and we pass that value to our activation function [Relu ==> max(0, x) = 0], it automatically gives 0 and no learning occur and no uodate. Which means, old wieght is equal to new weight.\n",
    "##### If learning rate is high, then weights become negative and again dying relu problem.\n",
    "##### Dying relu problem may also occur due to high bias.\n",
    "##### If neuron is dead, it means it is permenenty.\n",
    "\n",
    "##### Solution:\n",
    "##### Set slow learning rate.\n",
    "##### Bias should positive value(0.01)\n",
    "##### Dont use relu, use its variants.\n",
    "\n",
    "##### Leaky Relu:\n",
    "##### Unlike standard ReLU, which outputs 0 for all negative inputs, Leaky ReLU allows a small, non-zero gradient for negative values.\n",
    "##### f(x) = max(a.x, x), where α is a small positive constant (e.g., 0.01).\n",
    "\n",
    "##### Advantage:\n",
    "##### Easily Computed.\n",
    "##### No dying relu problem.\n",
    "##### Close to '0' centered and it can have negative value.\n",
    "\n",
    "##### Parametric ReLU:\n",
    "##### PReLU takes Leaky ReLU a step further by making the α parameter trainable through backpropagation, meaning the network learns the optimal slope for negative inputs for each neuron\n",
    "##### f(x) = max(ai.x, x), where ai is a learnable parameter.\n",
    "\n",
    "##### Exponential Linear Unit (ELU):\n",
    "##### ELU behaves like ReLU for positive inputs but for negative inputs, it smoothly approaches −α using an exponential function.\n",
    "##### f(x) = {x if x > 0 and a(exp(x) - 1) if x <= 0}, where a is positive constant and often set to '1'.\n",
    "\n",
    "##### Scaled Exponential Linear Unit (SELU):\n",
    "##### A variation of ELU designed to induce \"self-normalizing\" properties in neural networks. This means that if the network's weights are initialized correctly (using Lecun normal initialization), the activations in each layer will automatically converge towards a mean of 0 and a variance of 1, even in very deep networks.\n",
    "##### f(x) = λ {x if x > 0 and a(exp(x) - 1) if x <= 0}, where α≈1.6733 and λ≈1.0507 are fixed constants.\n",
    "\n",
    "##### Problem Solved:\n",
    "##### Vanishing/Exploding Gradients.\n",
    "##### Eliminates Need for Batch Normalization.\n",
    "##### Faster and More Stable Training.\n",
    "\n",
    "##### Weight Initialization Techniques:\n",
    "##### If we init very big number it causes saturation when using sigmoid and tanh.\n",
    "##### Saturated means highest positive and negative value.\n",
    "##### Saturation makes training slow.\n",
    "##### If small values then we getvery small values. And when we give this small value to tanh it stays closely to zero.\n",
    "\n",
    "##### Xavier/Glorot Initialization:\n",
    "##### It maintain a consistent variance of activations across all layers in the network, both during the forward pass and the backward pass. This helps prevent the activations and gradients from becoming too small (vanishing) or too large (exploding).\n",
    "##### When to use: Networks using sigmoid or tanh activation functions.\n",
    "\n",
    "##### He initialization:\n",
    "##### ReLU outputs 0 for negative inputs, which can halve the variance of activations in a layer. He initialization compensates for this by using a larger scaling factor.\n",
    "##### When to use: Networks using ReLU or its variants (Leaky ReLU, PReLU, ELU, SELU) as activation functions.\n",
    "\n",
    "#### Optimizers\n",
    "##### 1- Momentum:\n",
    "##### Instead of just taking a step in the direction of the current gradient, it also considers the direction of the previous steps. This helps the optimizer \"build up speed\" in consistent directions and \"smooth out\" oscillations\n",
    "\n",
    "##### Advantage:\n",
    "##### Faster Convergence.\n",
    "##### Escaping Local Minima.\n",
    "##### Oscillations in SGD.\n",
    "\n",
    "##### Dis-Advantage:\n",
    "##### Requires careful tuning of the momentum coefficient, can overshoot the optimal solution if momentum is too high.\n",
    "\n",
    "##### 2- AdaGrad (Adaptive Gradient Algorithm):\n",
    "##### AdaGrad is an adaptive learning rate optimizer. It changes learning rate during each epoch.\n",
    "##### It is useful when features are not on same scale and have big differences.\n",
    "##### Also when features has sparse data means most of single feature value is '0'.\n",
    "##### Due to sparse column, its update becomes small. That's why it doesn't come directly to global minima, but where sparse is not it move normally. \n",
    "##### For every step, if update is big then small learning rate and vice versa.\n",
    "##### It is not for complex NN, because it can't converge to global minima. Good for sparse data problems (e.g., NLP).\n",
    "\n",
    "##### Advantage:\n",
    "##### Automatic learning rate tuning.\n",
    "##### Different Feature Frequencies.\n",
    "\n",
    "##### Disadvantages:\n",
    "##### Decreasing Learning Rate.\n",
    "\n",
    "##### 3- RMSProp (Root Mean Square Propagation):\n",
    "##### RMSProp prevents the learning rate from becoming too small too quickly, allowing training to continue for longer and reach better minima.\n",
    "##### It solves the rapidly diminishing learning rate problem of AdaGrad.\n",
    "##### It forget gradually the old gradients and prefer latest gradient value.\n",
    "\n",
    "##### 4- Adam (Adaptive Moment Estimation):\n",
    "##### It combines RMSProp (adaptive learning rates based on squared gradients) and Momentum (incorporating past gradients for smooth updates).\n",
    "\n",
    "##### Advantage:\n",
    "##### Fast convergence.\n",
    "##### Efficient and Robust.\n",
    "##### Less Manual Tuning.\n",
    "\n",
    "#### Pooling: \n",
    "##### It is  also known as downsampling or sub-sampling, is a crucial operation in CNN.\n",
    "##### Decrease the width and height of the feature maps, which helps in reducing the computational cost and memory usage of the network.\n",
    "##### It helps not to depend on the location of pixel/feature.\n",
    "##### Extract Dominant Features.\n",
    "##### Control Overfitting.\n",
    "\n",
    "##### By applying max pooling we get translation invariance(it's not important where is feature, important is this that we have feature), because of eliminating low level details and pick dominant features.\n",
    "##### For every filter we have bias value and it is added in feature map(the result we get after applying filter).\n",
    "##### In Global Max Pooling, instead of doing flatten our data we send global max pool to avoid overfitting.\n",
    "\n",
    "\n",
    "#### Overview:\n",
    "##### Convolution Layers means, we pass images through filters, it extract primitive features like edges. After passing second layer it combines those into better presentation and moving towards it combine all and extract whole image.\n",
    "##### Edges: It means change in intensity. Means from black pixel to white pixel the change point is edge.\n",
    "##### Filters can apply from left to right, top to bottom. Values of filter are initializad randomly and during back propagation it update.\n",
    "##### If stride is big, means it only extract high features, if it is small it means it extract low features.\n",
    "##### When we got dense layer we flatten them and pass to fully connected layer/layers and finally output.\n",
    "##### In CNN it does not care about image size it on depend on filter size.\n",
    "##### In case of ANN it depends on input size.\n",
    "##### In max pooling while doing backward propagation we reshape [2 x 2] into again [4 x 4] in such a way that rest take zero.\n",
    "\n",
    "#### Pretrained Model: \n",
    "##### ImageNet Datasset:\n",
    "##### It consist of 100 classes of images from human to animal, foods, vehicles, chairs and many more.\n",
    "\n",
    "#### Transfer Learning :\n",
    "##### A ML technique focus on storing knowledge gained while solving 1 problem and applying it to different but related problems. \n",
    "##### In this we freeze convolution layer and only train fully connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170f05b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
