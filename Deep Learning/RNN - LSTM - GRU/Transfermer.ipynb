{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a250684",
   "metadata": {},
   "source": [
    "### Transformers:\n",
    "\n",
    "\n",
    "\n",
    "#### Core Concept: Attention Mechanism: \n",
    "\n",
    "##### In this instead of treating all words equally we pay attention to certain words or phrases that are most relevant to the sentence you're currently trying to understand.\n",
    "\n",
    "##### It allows an AI model to dynamically weigh the importance of different parts of its input data when processing information. Instead of treating all input equally, it learns to \"focus\" on the most relevant pieces for a given task or a specific part of its output.\n",
    "\n",
    "##### 1- \"Asking a Question\" (Query):\n",
    "\n",
    "##### It's trying to figure out how relevant other tokens are to understanding or generating something related to this current token.\n",
    "\n",
    "##### 2- \"Comparing\" (Keys): \n",
    "\n",
    "##### The model compares its \"query\" to all the \"keys\" of the other tokens. The more similar the query is to a key, the more relevant that other token is.\n",
    "\n",
    "##### 3- \"Getting the Information\" (Values):\n",
    "\n",
    "##### Each token also has a \"value\" vector, which contains the actual information or meaning of that token.\n",
    "\n",
    "\n",
    "\n",
    "##### Weighted Sum: \n",
    "\n",
    "##### Based on the similarity scores (how well the query matched the keys), the model calculates \"attention weights.\" These weights determine how much \"attention\" or importance to give to each of the \"value\" vectors. Finally, it creates a new representation for the current token by taking a weighted sum of all the \"value\" vectors.\n",
    "\n",
    "\n",
    "\n",
    "#### Transformers: \n",
    "\n",
    "##### Basically used to transform sequence to sequence.\n",
    "\n",
    "##### It is able to process data parallelly.\n",
    "\n",
    "##### Instead of LSTM, we use self attention method which is good for large inputs, because they process data parallelly.\n",
    "\n",
    "##### Removing LSTM and using self attention speed up training.\n",
    "\n",
    "##### GPT and BERT are transformer types. They are pre-trained but we can fine tune on our dataset.\n",
    "\n",
    "##### BERT uses transformer's encoder part and GPT uses only transformer's decoder part.\n",
    "\n",
    "##### We can import these from hugging face.\n",
    "\n",
    "##### Generative Accuracy Networks(GAN): Used for image generation.\n",
    "\n",
    "\n",
    "\n",
    "#### Advantage: \n",
    "\n",
    "##### Scalability --> Attention based, train parallelly.\n",
    "\n",
    "##### Transfer Learning\n",
    "\n",
    "##### Multi-Model(input/output)\n",
    "\n",
    "##### Flexible Architecture(Encoder and Decoder).\n",
    "\n",
    "\n",
    "\n",
    "#### Disadvantage:\n",
    "\n",
    "##### Computational Cost (GPU's)\n",
    "\n",
    "##### Data(LLMs) --> Text --> Un-supervised\n",
    "\n",
    "##### It is a black-box model.\n",
    "\n",
    "\n",
    "\n",
    "#### Encoder: \n",
    "\n",
    "##### It is used to process inputs.\n",
    "\n",
    "##### During training when we feed any prompts, first it tokenized the prompt, then it make vocabulary by counting unique words and give them indexes, after that it convert each words into embeddings, in embeddings it also add positional vector which tells us the position of the word.\n",
    "\n",
    "##### Embeddings are being made by contextual meaning of words. After making a contextual representation we give it to decoder part.\n",
    "\n",
    "\n",
    "\n",
    "##### While on inference time, we can't make vocabulary, because it is the part for training because our model is trained by all possible words we used.\n",
    "\n",
    "\n",
    "\n",
    "#### Decoder:\n",
    "\n",
    "##### It is used to generate output by processing the input of encoder.\n",
    "\n",
    "##### During training, we get the the embeddings from encoder, we first send <start> then first word, it gives output, it might be possible it gives wrong output, that's why in training instead of the output we send for t=2 we send the actual and correct output which comes from encoder part.\n",
    "\n",
    "##### But on inference time we directly send each output to next timestamp as input to generate next output.\n",
    "\n",
    "##### We have SoftMax on each output layer.\n",
    "\n",
    "\n",
    "\n",
    "##### So during training the actual process looks like this:\n",
    "\n",
    "##### Initial Data --> Forward Propagation --> Output --> Calculate Loss --> Gradients --> Update Weights --> Learning Rate --> New Weights --> Again Initial Data.\n",
    "\n",
    "\n",
    "\n",
    "#### Techer Forcing:\n",
    "\n",
    "##### In timestamp t=2, during training we pass actual output as input to next prediction instead of actual output which model predicts.\n",
    "\n",
    "\n",
    "\n",
    "#### Self Attention: \n",
    "\n",
    "##### Word embeddings has ability to capture semantic meaning of words.\n",
    "\n",
    "##### Each dimension has meaning but we don't know, only machine knows.\n",
    "\n",
    "##### Word embeddings depends on dataset context and it tell us average meaning. Example Apple fruit comes in dataset 9000 and it gives 0.9% probability while apply mobile is 1000 and it gives 0.3 probability.\n",
    "\n",
    "##### But we need contextual meaning, so self attention help us to capture that.\n",
    "\n",
    "\n",
    "\n",
    "##### Self attention tell us how words within a single sentence relate to each other.\n",
    "\n",
    "##### In self attention, the previous embeddings pass to self attention and it create new embedding for given data and create smart contextual meaning.\n",
    "\n",
    "##### Dot product tell us how similar are two vectors\n",
    "\n",
    "##### Query (Q): Asks question how similar it is to other.\n",
    "\n",
    "##### Key (k): It replies, this much it similar.\n",
    "\n",
    "##### Values (V): It contains values.\n",
    "\n",
    "##### We called it as self attention because it calculate similarity score in same sequence.\n",
    "\n",
    "\n",
    "\n",
    "#### --> In Scaled Dot Product Attention: \n",
    "\n",
    "##### High Variance is a problem so we overcome variance by dividing with dimension. \n",
    "\n",
    "##### So, when we apply SoftMax, it make small value to 1% and big values to 99%. \n",
    "\n",
    "##### Due to which small values neglect and big values only updated in back propagation.\n",
    "\n",
    "\n",
    "\n",
    "#### Multi-head Attention:\n",
    "\n",
    "##### Problem with self attention is, when it comes to translate a sentence which has two meanings, then self attention fail to capture both and only 1.\n",
    "\n",
    "##### By using 2 self attention, it capture two different meanings and we used the exact meaning.\n",
    "\n",
    "\n",
    "\n",
    "#### Positional Encoding:\n",
    "\n",
    "##### Self attention can calculate the contextual meaning in parallel, but it can't remember the order of words.\n",
    "\n",
    "##### So, it help us to remember the order of words.\n",
    "\n",
    "##### Positional Encoding is a vector whose dimension is always equal to dimension of embeddings vector.\n",
    "\n",
    "##### We add position vector to embedding vector.\n",
    "\n",
    "##### We don't concatenate it because it goes in big dimension.\n",
    "\n",
    "\n",
    "\n",
    "#### Layer Normalization: \n",
    "\n",
    "##### Improved training stability, by reducing extreme value to small values.\n",
    "\n",
    "##### Faster Convergence:\n",
    "\n",
    "##### By memorizing inputs and activation function model can converge more quickly because gradient(rate of change of the loss function with respect to the weights and biases) have more consistent magnitude.\n",
    "\n",
    "##### We don't use batch normalization, because in a batch size (8, 16, 32) if the longest sentence has 100 words and rest have very few we need to add zero padding. And we take standard aviation, due to lots of zero's the standard aviation don't kind of work.\n",
    "\n",
    "##### In batch, we take mean from top to bottom across batch but in layer normalization we do left to right.\n",
    "\n",
    "\n",
    "\n",
    "#### Transformer Encoder:\n",
    "\n",
    "##### Consist of 2 things: \n",
    "\n",
    "##### 1- Feed forward NN\n",
    "\n",
    "##### 2- Self Attention\n",
    "\n",
    "\n",
    "\n",
    "##### There are 6 encoders and decoders in transformer architecture.\n",
    "\n",
    "\n",
    "\n",
    "##### Input --> Tokenizer --> Embedding(512) + Positional Vector(512) --> Actual input(How are you?) --> Encoder [in encoder when pass with self attention it gives vector] --> Actual input + Encoder --> New Vector --> Layer Normalization --> Feed Forward [we give vector to NN it goes low to high dimension and at the end from high dimension to low dimension] --> Add and Normalization\n",
    "\n",
    "##### Parameters in each encoder block is totally different.\n",
    "\n",
    "\n",
    "\n",
    "#### Transformer Decoder:\n",
    "\n",
    "##### It behave auto-regressive at inference time and non-auto regressive at training time.\n",
    "\n",
    "##### Auto-Regressive Model: \n",
    "\n",
    "##### A class of model that generate data points in sequence, and the new point is dependent on the previous output.\n",
    "\n",
    "##### Encoder can process parallelly during training and inference time but decoder only during training time.\n",
    "\n",
    "\n",
    "\n",
    "#### Cross Attention:\n",
    "\n",
    "##### From encoder part, inputs comes and directly goes to decoder multi-head attention part.\n",
    "\n",
    "##### When we find similarity in same sequence then we use self-attention but when two different sequence we use cross attention(English and its translated Urdu sequence).\n",
    "\n",
    "##### In cross attention we need both English and Urdu embeddings to multi-head.\n",
    "\n",
    "##### By applying Relu we get non-linearity. The reason to get non-linearity is to train the model to capture non-linear patterns because in real world we have non-linear data.\n",
    "\n",
    "##### After normalization, we count unique words and that unique words becomes total number of neurons in a hidden layer.\n",
    "\n",
    "##### A 512 dimension of vector pass through and apply SoftMax."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
