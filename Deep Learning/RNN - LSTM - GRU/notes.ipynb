{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bee187",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN) \n",
    "##### It process sequential data, where the order of elements is crucial. Unlike traditional neural networks, RNNs have a \"memory\" that allows them to consider previous inputs when processing current inputs, making them suitable for tasks like NLP, speech recognition, and time series analysis. \n",
    "##### It is used for Image captioning, Translation, Sentiment analysis( bad/good reviews) \n",
    "##### In RNN we feed one by one because it works on sequence to sequence data.\n",
    "##### When we feed something to RNN it first tokenized each word then make a vaublary. Like counting unique words and give them some index. After that we apply OHE and it convert into vectors.\n",
    "##### When timestamp t=1, we feed our first word to hidden layer, each hidden layer generate output, and we give that output also to next hidden layer as input with next word. \n",
    "\n",
    "#### Disadvantage:\n",
    "##### Text input size vary.\n",
    "##### Due to zero padding unnecessary computation.\n",
    "\n",
    "#### Problems:\n",
    "##### It start to forget old things, because it tries to remember every word, but due to memory limitations it can't remember start words.\n",
    "##### It face Vanishing and exploiding gradient descent problem.\n",
    "##### Suppose we have 100 words of vacublary, when it cross timestamp 30 or we can say it reach t=90, due to derivative of start words come closely to zero and the closest word has high value which contribute in prediction more. We use tanh in this, so derivative become(0,1).\n",
    "##### If we use ReLU, the derivative is always positive number and first word always dominate.\n",
    "\n",
    "\n",
    "### Long Short-Term Memory (LSTM);\n",
    "##### It is designed to handle sequential data and address the vanishing gradient problem that can hinder traditional RNNs.\n",
    "##### It has three gates.\n",
    "##### Input Gate.\n",
    "##### Output Gate.\n",
    "##### Forget Gate.\n",
    "\n",
    "##### Architecture:\n",
    "##### f_t, i_t, c_t, o_t\n",
    "##### f_t has power to pass information of C_t-1. Either it is 0, 50 or 100%.\n",
    "##### Input Stage: c_t candidate cell state, based on current inputand previous hidden state, it search some new value to add in LSTM.\n",
    "##### It filter from c_t and add to cell state.\n",
    "\n",
    "\n",
    "### Gated Recurrent Units (GRUs):\n",
    "##### It is a type of (RNN) designed to address the vanishing gradient problem and improve performance on sequence data tasks.\n",
    "##### It is also used for sequential data.\n",
    "##### It requires less training, less parameters to train, less complex than LSTM.\n",
    "##### It consist of two gates.\n",
    "##### Update gate: This gate controls how much of the previous hidden state should be passed on to the next time step. \n",
    "##### Reset gate: This gate determines how much of the past information should be discarded when updating the current state. \n",
    "\n",
    "##### Architecture:\n",
    "##### h_t-1 ---> Previous hidden state.\n",
    "##### h_t ---> current state.\n",
    "##### x_t ---> input.\n",
    "##### r_t ---> reset gate.\n",
    "##### z_t ---> update gatae.\n",
    "##### h_t --->  candidate hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e93dab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
