{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a811021",
   "metadata": {},
   "source": [
    "### Deep learning \n",
    "##### It works like human brain due to neurons, don't need human interventinos because it learn from its own environment.\n",
    "#### Difference between Machine Learning Vs Deep Learning:\n",
    "##### For ML it can train on small data but DL requires large data.\n",
    "##### ML requires more human intervention to correct and learn\twhere as DL learns on its own from environment and past mistakes.\n",
    "##### ML Shorter training and lower accuracy where as DL Longer training and higher accuracy.\n",
    "##### In ML data increase may not increase accuracy where as in DL larger the data means larger accuracy.\n",
    "\n",
    "#### Perceptron:\n",
    "##### Perceptron is nothing but a line. It is the simplest form of a neural network, a single neuron (or processing unit). It gives single output, but on case of Multi-Layer Perceptron(MLP's) it generates multiple output.\n",
    "##### It only can work on linear data. It is a supervised learning algorithm.\n",
    "##### Advantages:\n",
    "##### It's used for binary classification where a straight line draw to distinguish two different classes.\n",
    "##### Disadvantages:\n",
    "##### It famously can't solve the XOR (non-linearly separable problems) problem. It only work with linear data.\n",
    "\n",
    "##### Perceptron Trick:\n",
    "##### Start with a random line which says right side is 1 and left side is 0.\n",
    "##### Then it starts to check each point, if point/data is on right side it check another point. If data is not on right side the line rotate and bring it another side.\n",
    "\n",
    "##### To identify right side is positive and left side is negative, we put data values(x,y) in to Ax + By + C = 0. \n",
    "##### if data > 0 positive.\n",
    "##### if data < 0 negative.\n",
    "##### if data = 0 data is on the line / above line.\n",
    "\n",
    "##### If negative point is on positive side, then at the end of cordinates we subtract 1 from line coefficient and vice versa. \n",
    "##### Instead of directly subtract cordinates we multiply cordinates with learning rate then subtract to avaid big jumps.\n",
    "##### Perceptron is somehow equal to Logistic regresssion in a sence if and only if \"activation finction is sigmoid\" and \" loss function is binary cross entropy\".\n",
    "##### Loss Functions: Hinge Loss, Binary Cross Entropy, Categorical Cross Entropy, Mean Square Error.\n",
    "##### Activation Function: Step, Sigmoid, Softmax, Linear.\n",
    "##### Output: Perceptron, Logistic Regression, Multiclass, Linear Regression.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
