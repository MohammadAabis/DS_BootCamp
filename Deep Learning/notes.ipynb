{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a811021",
   "metadata": {},
   "source": [
    "### Deep learning \n",
    "##### It works like human brain due to neurons, don't need human interventinos because it learn from its own environment.\n",
    "#### Difference between Machine Learning Vs Deep Learning:\n",
    "##### For ML it can train on small data but DL requires large data.\n",
    "##### ML requires more human intervention to correct and learn\twhere as DL learns on its own from environment and past mistakes.\n",
    "##### ML Shorter training and lower accuracy where as DL Longer training and higher accuracy.\n",
    "##### In ML data increase may not increase accuracy where as in DL larger the data means larger accuracy.\n",
    "\n",
    "#### Perceptron:\n",
    "##### Perceptron is nothing but a line. It is the simplest form of a neural network, a single neuron (or processing unit). It gives single output, but on case of Multi-Layer Perceptron(MLP's) it generates multiple output.\n",
    "##### It only can work on linear data. It is a supervised learning algorithm.\n",
    "##### Advantages:\n",
    "##### It's used for binary classification where a straight line draw to distinguish two different classes.\n",
    "##### Disadvantages:\n",
    "##### It famously can't solve the XOR (non-linearly separable problems) problem. It only work with linear data.\n",
    "\n",
    "##### Perceptron Trick:\n",
    "##### Start with a random line which says right side is 1 and left side is 0.\n",
    "##### Then it starts to check each point, if point/data is on right side it check another point. If data is not on right side the line rotate and bring it another side.\n",
    "\n",
    "##### To identify right side is positive and left side is negative, we put data values(x,y) in to Ax + By + C = 0. \n",
    "##### if data > 0 positive.\n",
    "##### if data < 0 negative.\n",
    "##### if data = 0 data is on the line / above line.\n",
    "\n",
    "##### If negative point is on positive side, then at the end of cordinates we subtract 1 from line coefficient and vice versa. \n",
    "##### Instead of directly subtract cordinates we multiply cordinates with learning rate then subtract to avaid big jumps.\n",
    "##### Perceptron is somehow equal to Logistic regresssion in a sence if and only if \"activation finction is sigmoid\" and \" loss function is binary cross entropy\".\n",
    "##### Loss Functions: Hinge Loss, Binary Cross Entropy, Categorical Cross Entropy, Mean Square Error.\n",
    "##### Activation Function: Step, Sigmoid, Softmax, Linear.\n",
    "##### Output: Perceptron, Logistic Regression, Multiclass, Linear Regression.\n",
    "\n",
    "#### Forward Propagation | How Neural Network Predicts\n",
    "##### The process where input data is passed through the network, layer by layer, to produce an output.\n",
    "##### On each node of layers we apply activation function.\n",
    "##### Training and validation accuracy should increase. If training accuracy increase and validation accuracy don't it means overfitting.\n",
    "##### If there is outliers then we use Mean Absolute Error as a loss function.\n",
    "##### If there is no outliers then we use Mean Square Error as a loss function.\n",
    "\n",
    "#### Backward Propagation \n",
    "##### After model's prediction we compare it with actual target and calculate loss functions.\n",
    "##### After that we apply Gradient Descent technique to update weights layer by layer.\n",
    "##### Weights update on chain rule.\n",
    "\n",
    "#### Vanishing Gradient Descent:\n",
    "##### The vanishing gradient problem occurs when the gradients of the loss function with respect to the weights in the earlier layers of a deep network become extremely small or no change in weights. \n",
    "##### In backward propagation, the weights updates depends on partial derivative. Since, in DNN the derivative we got very small and when we subtract from old weight it give very minor change.\n",
    "##### Impact:\n",
    "##### Slow convergence. \n",
    "##### Earlier layers learn much slower than later layers, or not at all.\n",
    "\n",
    "#### Exploding Gradients: \n",
    "##### The exploding gradient problem is the opposite of vanishing gradients. It occurs when the gradients become excessively large during backpropagation.\n",
    "##### If the derivatives in the backpropagation process are consistently large (greater than 1), their product can grow exponentially as it propagates backward. This results in huge updates to the network weights.\n",
    "##### Impact:\n",
    "##### The training process becomes unstable.\n",
    "##### Large weight updates can cause the model to overshoot optimal solutions in the weight space.\n",
    "\n",
    "#### How to avoid Vanishing and Exploding Gradients Problem: \n",
    "##### 1- Careful Weight Initialization: \n",
    "##### Don't use zeros or small random numbers. Instead we can use 'Xavier', 'He Initialization' techniques for weghts initialization.\n",
    "##### 2- Choosing Non-saturating Activation Functions:\n",
    "##### Use activation functions whose derivatives do not become excessively small (saturate) over large input ranges. Like ReLU, Leaky ReLU. Avoid sigmoid and tanh. \n",
    "##### 3- Gradient Clipping (Primarily for Exploding Gradients):\n",
    "##### To prevent gradients from becoming too large by \"clipping\" them if they exceed a predefined threshold. During backpropagation, if it surpasses a certain threshold, scale it down to be within that threshold. This doesn't change the direction of the gradient but limits its magnitude, preventing overly large weight updates. \n",
    "##### 4- Batch Normalization:\n",
    "##### 5- Residual Networks (ResNets):\n",
    "##### To enable the training of much deeper networks by introducing \"skip connections\" or \"shortcuts.\"\n",
    "##### The output of a layer is added to its input. This allows gradients to be directly backpropagated through these skip connections to earlier layers, bypassing some intermediate layers. This direct path helps to prevent gradients from vanishing as they propagate through the depth of the network.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252cf5e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
