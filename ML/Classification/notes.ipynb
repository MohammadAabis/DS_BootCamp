{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6058cacf",
   "metadata": {},
   "source": [
    "### Classification\n",
    "##### It is a type of supervised learning where the goal is to predict the categorical class labels of new instances, based on past observations.\n",
    "\n",
    "#### Common Classification Problem\n",
    "##### 1. K-Nearest Neighbors (KNN)\n",
    "##### 2. Support Vector Machines (SVM)\n",
    "##### 3. Decision Tree\n",
    "##### 4. Random Forest Tree\n",
    "##### 5. Naive Bayes\n",
    "\n",
    "#### 1. K-Nearest Neighbors (KNN)\n",
    "##### Select value for k (e.g., 3, 5, 7). Should be odd otherwise tie happened in even values. \n",
    "##### KNN calculates the \"distance\" (e.g., Euclidean distance) between the nearest one.  \n",
    "##### Elbow Method is used to find best value of K.\n",
    "##### No effect of time while calculating the distance if we have two different patterns in which first have data close to each other and second has larger distance.\n",
    "\n",
    "#### Usage of KNN:\n",
    "##### 1- Recommendation Systems:\n",
    "##### 2- Spam Detection.\n",
    "\n",
    "#### Algos which solves KNN problems.\n",
    "##### 1- Decision Tree\n",
    "##### 2- Random Forest Tree\n",
    "\n",
    "#### Drawbacks of KNN:\n",
    "##### Computationally Expensive: Calculating distances to all training points can be slow, especially with large datasets.\n",
    "##### Curse of Dimensionality: KNN performs poorly when you have a very large number of features (dimensions).\n",
    "##### A small (high variance, low bias) K can make the model sensitive to noise, while a large (high bias, low variance) K can make the decision boundary too smooth and less precise.\n",
    "\n",
    "\n",
    "#### 2. Support Vector Machines (SVM)\n",
    "##### It is powerful supervised learning models used for classification and regression tasks.\n",
    "##### SVM tries to find the maximum possible margin (empty space) between the hyperplane and the closest points from each group.\n",
    "##### The \"best\" hyperplane is the one that has the largest distance to the nearest data points of any class.\n",
    "##### SVM works well even when the number of features is greater than the number of samples.\n",
    "##### Support Vectors: Where π positive and π negative touch the first point/sample.\n",
    "##### Robust to outliers and able to handle.\n",
    "##### Can work with linear and non-linear data.\n",
    "##### Polynomial Kernel: This kernel maps data into a polynomial feature space of degree d. It can find polynomial decision boundaries.\n",
    "##### Magnitude is 1 and -1 because our hyperplane (π) should at center.\n",
    "\n",
    "#### Usage of SVM:\n",
    "##### 1- Face Detection\n",
    "##### 1- Spam Detection.\n",
    "\n",
    "#### Drawbacks of SVM:\n",
    "##### If the classes are heavily overlapping, SVM might not find a clear separating hyperplane, or the margin might be very small.\n",
    "\n",
    "\n",
    "#### 3. Decision Tree:\n",
    "##### The tree starts with a \"root node\" representing the entire dataset.\n",
    "##### Find the Best Split: The algorithm searches for the feature and the threshold value for that feature that best splits the data into more homogeneous subgroups\n",
    "\n",
    "#### Drawbacks of DT:\n",
    "##### Instability: Small changes in the data can lead to a completely different tree structure.\n",
    "##### If one class is much more frequent than others, the tree might be biased towards predicting that class.\n",
    "\n",
    "\n",
    "#### 4. Random Forest Tree:\n",
    "##### A group of decision tree train at same time.\n",
    "##### Can used as both classification and regression.\n",
    "##### Don't need to tune mask, without touching parameters it gives you best performance.\n",
    "##### Data Selection: with replacement selection of data can duplicate but without replacement data can't duplicate. Also same for feature(column) sampling.\n",
    "##### Due to giving random data the shape may change at end of training.\n",
    "##### For classification: When prediction gives by deccision trees, we take aggregation and then select class for that.\n",
    "##### Generally the relation of bias and variance is V/B but due to randomness random forest convert high bias and low variance into low variance and low bias.\n",
    "##### Due to distribution of outlayers and noices to different trees the variance is low.\n",
    "##### In random forest all base algorithm are decision tree but in bagging it used different algorithm.\n",
    "##### In bagging if all algorithm are same it can't equal to random forest because in bagging when doing row/column sampling if we set limit as 2 we know first node will become from random sample/feature and second node from another sample/feature.\n",
    "##### But in random forest due to randomness it always randomly pick samples/features for every single node creation.\n",
    "\n",
    "#### Drawbacks of RFT:\n",
    "##### Training many trees takes more time and computational resources than training a single tree.\n",
    "##### Excessive noise can still lead to overfitting if parameters like the number of trees or tree depth are not carefully chosen.\n",
    "\n",
    "\n",
    "#### 5. Naive Bayes\n",
    "##### It says that the features within a dataset are independent of each other, given the class label. \n",
    "##### It performs remarkably well in practice, particularly with text-based data and large datasets, due to its simplicity and computational efficiency.\n",
    "\n",
    "#### Usages\n",
    "##### Spam Filtering: Identifying whether an email is spam or not based on the presence of certain words or patterns.\n",
    "##### Text Classification: Categorizing documents into different topics (e.g., sports, politics, technology) based on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a2c61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33014a3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
