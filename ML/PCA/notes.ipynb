{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87a4e03",
   "metadata": {},
   "source": [
    "### Principal Component Analysis(PCA)\n",
    "##### It's an un-supervised ML technique used for feature extraction which means transform high dimensional data into lower dimensional data while keeping the actual data.\n",
    "##### Curse of dimensionality: It means it reduces features. Like from 3D to 2D.\n",
    "##### It is good for visualization after reducing features.\n",
    "\n",
    "#### Feature Extraction:  \n",
    "##### Pca makes new subset of features in which it stores only important features.\n",
    "##### We shift our features in different axis and if our feature name is \"size\" then it becomes after shifting \"size'\". d > d'\n",
    "##### It choose high variance at first as pc1 and so on till the sum of variance is 90.\n",
    "##### Variance tell us how much data points in a dataset are spread out from their mean value. Higher the variance means it has importance.\n",
    "\n",
    "#### Why variance is important?\n",
    "##### Variance is directly proportional to spread of data.\n",
    "##### Maximize the variance because from low to high dimensional variance should same.\n",
    "\n",
    "#### Covariance and Covariance matrix: \n",
    "##### It tell us how data changed their axis with each other and now how they behave.\n",
    "##### Covariance matrix = | var X_1 |           | CV(X_2,X_1) |\n",
    "#####                     | CV(X_1,X_2) |      | var X_2 |\n",
    "\n",
    "##### Covariance: It tells us the relationship between two points. Positive covariance means they are in same direction and negative means they are in opposite direction and if zero there is no relationship.\n",
    "##### Correlation: It is measure of the strength and direction of the linear relationship between two variables and ranges between -1 and 1. And rest is above.\n",
    "\n",
    "##### Eigen Vectors: \n",
    "##### When we apply a transformation (like stretching, shrinking, or rotating) to it, it stays pointing in the same direction. It only gets scaled. \n",
    "##### An uncorrelated directions that capture the most variance in your data. They are the directions along which your data is most spread.\n",
    "##### Eigen Values:\n",
    "##### This is the number that tells you how much the eigenvector was stretched or shrunk (or flipped). If the eigenvalue is 2, the eigenvector doubled in length. If it's 0.5, it shrunk to half its length. If it's -1, it flipped to point in the exact opposite direction but kept its original length.\n",
    "\n",
    "#### Steps: \n",
    "##### Mean Clustering. bring the data near to center. \n",
    "##### Find Covariance matrix.\n",
    "##### Find eigen values.\n",
    "##### Transform: Apply standard scaling then find covariance and find Eigen values on Eigen vectors.\n",
    "##### If we have 784 features ==> λ_1, λ_2 ... λ_n,\n",
    "##### Each of the single vector (λ_1) tell that how much it explain the variance of oiginal data. (λ_1 / λ_1, λ_2 ... λ_n) * 100\n",
    "##### When it explain 90% of variance, then we got better result for our data.\n",
    "\n",
    "####  Advantages of PCA:\n",
    "##### Reduces Dimensionality: Simplifies data and models, making them easier to work with and potentially faster to train.\n",
    "##### Data Visualization: Allows high-dimensional data to be visualized in 2D or 3D.\n",
    "\n",
    "####  Disadvantages/Limitations of PCA:\n",
    "##### Information Loss: Dimensionality reduction inherently leads to some information loss, although PCA tries to minimize this by preserving maximum variance.\n",
    "##### Sensitive to Data Scaling: PCA is highly sensitive to the scaling of the features. Standardization is crucial.\n",
    "##### Sensitive to Outliers: Outliers can disproportionately influence the calculation of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9db423",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
