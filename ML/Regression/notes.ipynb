{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c25e7de",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "##### Regression is a fundamental supervised learning technique in ML used for predicting a continuous target variable based on one or more input features.\n",
    "##### Unlike classification, which predicts categorical classes, regression predicts continuous values like price, temperature, or age.\n",
    "##### It finds a function that best describes the relationship between the independent variables (features) and the dependent variable(target).\n",
    "##### The goal of regression is finding a \"best-fit\" line or curve that can be used to make predictions on new, unseen data\n",
    "\n",
    "#### Supervised Machine Learning\n",
    "##### Uses labeled data (input features + corresponding outputs).\n",
    "##### Predicts outcomes or classifies data based on known labels.\n",
    "##### Less complex, as the model learns from labeled data with clear guidance.\n",
    "##### Model can be tested and evaluated using labeled test data.\n",
    "\n",
    "##### Example.\n",
    "##### Machine is given a dataset with input features (like age, salary, or temperature) and corresponding labels (like “yes/no,” “high/low,” or “rainy/sunny”).\n",
    "##### Then machine learns dataset by finding patterns in the data. For example, it might learn that if the temperature is high, it’s likely to be sunny.\n",
    "\n",
    "#### Un-Supervised Machine Learning\n",
    "##### Uses unlabeled data (only input features, no outputs).\n",
    "##### Discovers hidden patterns, structures, or groupings in data.\n",
    "##### More complex, as the model must find patterns without any guidance.\n",
    "##### Cannot be tested in the traditional sense, as there are no labels\n",
    "\n",
    "##### Example\n",
    "##### Imagine visiting a new city without a map or guide.\n",
    "##### Buildings with tall spires might be grouped as churches.\n",
    "##### Open spaces with greenery might be categorized as parks.\n",
    "##### Streets with lots of shops could be grouped as markets.\n",
    "\n",
    "\n",
    "#### Underfitting:\n",
    "##### It occurs when a model is too simple to capture the underlying patterns in the data. It hasn't learned enough from the training data.\n",
    "#### Causes: Insufficient training data or Poor feature selection.\n",
    "##### Analogy: Like a student who didn't study enough for the exam. They only have a very basic understanding of the subject.\n",
    "\n",
    "\n",
    "#### Overfitting:\n",
    "##### It occurs when a model learns the training data too well, including the noise and random fluctuations, rather than the true patterns.\n",
    "#### Causes: Noisy training data or model is too complex.\n",
    "##### Analogy: Like a student who memorized every single practice question and answer but didn't truly understand the concepts. It will perfectly work on training data but on unseen data it don't.\n",
    "\n",
    "\n",
    "#### Common Regression Algorithms\n",
    "##### Linear Regression\n",
    "##### Polynomial Regression\n",
    "##### Logistic Regression\n",
    "##### Support Vector Regression\n",
    "##### Decision Tree Regression\n",
    "\n",
    "\n",
    "#### What Problems Regression is it Used For?\n",
    "##### Forecasting: Predicting future sales, stock prices, or demand for a product.\n",
    "##### Estimation: Estimating the price of a house.\n",
    "##### Healthcare: Predicting patient recovery time, disease progression, or the cost of treatment.\n",
    "\n",
    "\n",
    "#### Basic Concepts to Know?\n",
    "##### y = mx + b\n",
    "##### we can also say, m is coefficient and b is intercept.\n",
    "##### Y is target value, X is features independent values, m is the slope how steep the line is and b the position of slope line like where it is falling on x-axis, y-axis or in the middle.\n",
    "\n",
    "\n",
    "#### Loss Funtions\n",
    "##### Cost / Loss : Loss function is to capture the difference between the actual and predicted values for a single record whereas cost functions aggregate the difference for the entire training dataset.\n",
    "##### Cost Function (Loss Function): It tell us difference between the predicted values and the actual values of the target variable.\n",
    "##### Mean Absolute Error (MAE): Calc the average of the absolute differences between predicted and actual values.\n",
    "##### --> Advantage: Robust for outlayers.\n",
    "##### --> Dis-Advantage: Graph cann't be differentiable due to straight \"V\" shape.\n",
    "##### Mean Squared Error (MSE): Calculates the average of the squared differences between predicted and actual values.\n",
    "##### --> Advantage: Can use as loss function because graph can be differentiable.\n",
    "##### --> Dis-Advantage: The square for outliers is too big.\n",
    "##### Root Mean Squared Error (RMSE): The square root of the MSE, providing an error metric in the same units as the target variable.\n",
    "##### --> Advantage: It returns same unit after applying (LPA == LPA for Y^).\n",
    "##### --> Dis-Advantage: It is sensitive to oulayers, means the RMSE for outlayer is too high.\n",
    "##### Error(loss) = Submission( Y-actual(Yi) - Y-predicted(y^) ) == (Yi - MXi - b)\n",
    "\n",
    "\n",
    "\n",
    "#### Predictors\n",
    "##### R2 Score: It tells you how well your model explains the variation in the data.\n",
    "##### Bad --> (0 <-- R2 --> 1) <-- Good\n",
    "##### Misleading Impression: A model with many irrelevant features can appear to have a better fit (higher R2 ) than a simpler model with only relevant features.\n",
    "##### Adjusted R2 Score: This modifies the drawback of R2  by taking into account the number of independent variables in the model.\n",
    "\n",
    "\n",
    "#### Optimizers\n",
    "##### It is used for finding the best-fit line is minimizing the sum of squared errors.\n",
    "##### To find y = mx + b, we have two solutions.\n",
    "\n",
    "##### 1: Ordinary Least Squares (OLS): When data is small or in 1D.\n",
    "##### 2: Gradient Descent: For large data or more for than 1D data.\n",
    "\n",
    "##### Gradient descent: An optimization algorithm used in ML to find the best parameters for a model by iteratively adjusting them to minimize a cost function. \n",
    "##### when we got error we apply Gradient Descent to find minimum loss / local minima point which can suite for prediction.\n",
    "\n",
    "#### Steps:\n",
    "##### Before applying Gradient Descent the line we get will not the best-fit line. Due to random selection of m and b.\n",
    "##### Compute Gradients (slopes of loss w.r.t m and b)\n",
    "##### --> dL/dm = -(2/n) ∑ x_i(y_i - (mx_i + b))\n",
    "##### --> dL/db = -(2/n) ∑ (y_i - (mx_i + b))\n",
    "##### If slope is positive decrement in b value. If Slope is negative increment in b value.\n",
    "##### Update m and b: New m = Old m − learning_rate × dL/dm and New b = Old b − learning_rate × dL/db\n",
    "\n",
    "#### When to Stop:\n",
    "##### 1- Different between b_old and b_new is greater or equal(>=) to 0.001 \n",
    "##### 2- Iterative methods. 100 to 1000 epochs\n",
    "\n",
    "\n",
    "#### Types of Gradient Descent\n",
    "\n",
    "##### 1- Batch GD.\n",
    "#####  The loss function is calculated using the entire training dataset for each parameter update. This means that the model parameters are updated only once per epoch.\n",
    "##### Advantage:\n",
    "##### Using the entire dataset provides a true gradient direction, leading to precise parameter updates.\n",
    "##### For convex loss functions, BGD is guaranteed to converge to the global minimum with a suitable learning rate.\n",
    "##### Convex Loss Function: Which has a single, global minimum, making it easier for GD to converge to the optimal solution,\n",
    "##### Dis-Advantage:\n",
    "##### Bad for big data, because memory can't load whole data at a time.\n",
    "##### Processing the entire dataset for each update can be very slow and computationally intensive.\n",
    "\n",
    "##### 2- Stochastic GD.\n",
    "#####  It update the model parameters by row by row.\n",
    "##### Advantage:\n",
    "##### It is faster and it select random rows not sequentially\n",
    "##### Due to process one row at a time, making it memory-efficient and suitable for large datasets.\n",
    "##### The noisy updates due to processing one row at a time can help SGD jump out of shallow local minima and potentially find a better minimum.\n",
    "##### Dis-Advantage:\n",
    "##### It doesn't give steady solution but give solution near to global minima.\n",
    "##### Even reaching near global minima it shows impartiallity. To overcome this we have \" Learning Sechedule\" technique in which we change learning rate with each epochs change.\n",
    "\n",
    "##### 3- Mini Batch GD.\n",
    "#####  It divides the training dataset into small data.\n",
    "##### Advantage:\n",
    "##### It introduces some randomness that can help in escaping local minima.\n",
    "##### We set batch size and it compute that data then update weights.\n",
    "##### Itroduces the noise compared to SGD, leading to a smoother convergence path.\n",
    "##### Dis-Advantage:\n",
    "##### The size of the mini-batch is an additional hyperparameter that needs to be tuned.\n",
    "##### While less noisy than SGD, the convergence path is not as smooth as BGD.\n",
    "\n",
    "##### 4- Adam GD.\n",
    "#####  It is the combination idea of Momentum and RMSprop.\n",
    "##### It remembers the general downhill direction. If slope continues in one direction it increase learning rate through which we can take big steps. If slope change its direction it decrease its learning rate.\n",
    "##### Advantage:\n",
    "##### Adjusts the learning rate for each parameter individually, which can lead to faster convergence.\n",
    "##### Requires storing only two moving averages per parameter, making it memory-efficient.\n",
    "##### Each parameter has its own adaptive learning rate based on past gradients and their magnitudes.\n",
    "##### Dis-Advantage:\n",
    "##### While generally robust, performance can still be influenced by the initial learning rate and the β values.\n",
    " \n",
    "\n",
    "#### 2- Logistic Regression.\n",
    "##### It is primarily used for binary classification problems, predicting to a particular class ( 0 or 1).\n",
    "##### It is used when data is linear.\n",
    "##### The sigmoid function is used to map the output of a linear combination of inputs to a probability between 0 and 1. σ(z) = 1 / 1 + e^-z\n",
    "##### Less sensitive to outliers compared to linear regression due to the sigmoid function compressing the output, but extreme outliers can still have an influence.\n",
    "##### When it comes to multiclass we import Logistic regression and create instance for LogisticRegression. We need to specify like its multiclass problem in param LogisticRegression(multi_class='multinomial'). For more than 2 class we used \"softmax\" function.\n",
    "##### Softmax as activation function which then classify the output in which class it should go.\n",
    "##### We also have models like Support Vector Machines (SVMs), Random Forests, Decision Trees to solve this problem.\n",
    "\n",
    "\n",
    "#### Loss Functions:\n",
    "##### 1- Step Function: A function where the output value stays the same across an interval, and then \"jumps\" to a different constant value at the end of that interval.\n",
    "##### 2- Maximum Likelihood Estimation: It is about finding the model maximum value.\n",
    "##### 3- Cross Entropy: It is to find minimum value. Because we have to take log.\n",
    "##### The submission of negative log of maximum likelihood is cross entropy.\n",
    "\n",
    "\n",
    "#### Evaluation Metrics:\n",
    "##### 1- Confusion Matrix: It counts the correct and incorrect predictions.\n",
    "#####                   Predicted Positive\tPredicted Negative\n",
    "##### Actual Positive\tTrue Positive (TP)\tFalse Negative (FN)\n",
    "##### Actual Negative\tFalse Positive (FP)\tTrue Negative (TN)\n",
    "\n",
    "##### 2- Precision: Out of all the fruits your model said were red apples, how many were actually red apples?  P = TP / TP + FP.\n",
    "##### Focus: Minimizing False Positives (incorrectly labeling other fruits as red apples).\n",
    "\n",
    "##### 3- Recall: Out of all the actual red apples in the basket, how many did your model successfully find?    P = TP / TP + FN.\n",
    "##### Focus: Minimizing False Negatives (missing actual red apples).   \n",
    "\n",
    "##### 4- F1-Score: Provides a balance between Precision and Recall. 2 (PR/P+R)\n",
    "##### Concerned with: Achieving a good performance on both Precision and Recall simultaneously.\n",
    "\n",
    "\n",
    "#### 3- Polynomial Regression:\n",
    "##### It allows you to fit a curved line to your data. Use this when data is non-linear.\n",
    "##### The higher the \"degree\" of the polynomial (the highest power of x you use), the more flexible the curve can be.\n",
    "\n",
    "#### Regularization\n",
    "##### Try to fit the data well, but also try to keep your curve smooth by not letting your coefficients get too large.\n",
    "##### --> λ (lambda) is the regularization parameter.\n",
    "##### --> If λ=0, there is no penalty, and it behave like linear regression.\n",
    "##### --> If λ is large, the penalty is strong, forcing the coefficients to be very small.\n",
    "\n",
    "#### 1- Ridge Regularization(L2):\n",
    "##### This adds a penalty proportional to the square of the magnitude of the coefficients to the loss function.\n",
    "##### Penalty of three features looks like λ(x1^^2 + x2^^2 + x3^^2)\n",
    "##### Use when all features are important.\n",
    "##### If lambda value increase it nearly goes to zero but not zero.\n",
    "##### If coefficient has big value than the higher value decrease quickly.\n",
    "##### Lambda decrease --> bias decrease --> overfit --> variance increase.\n",
    "##### Lambda increase --> bias increase --> underfit --> variance decrese.\n",
    "##### alpha increse --> loss function decrease.\n",
    "\n",
    "#### 2- Lasso Regularization(L1):  λ×(∣β1∣ + ∣β2∣ + ∣β3∣)\n",
    "##### This adds a penalty proportional to the absolute value of the magnitude of the coefficients to the loss function.\n",
    "##### Use when all features are not important.\n",
    "##### If lambda value increase it goes to zero.\n",
    "##### It tell us which features are important by making un-important features to zero.\n",
    "##### It create sparsity, means that by increasing alpha some coefficient becomes zero.\n",
    "\n",
    "\n",
    "\n",
    "#### Drawbacks of Regression\n",
    "##### In regression models, outliers can mislead the model, making it learn an inaccurate relationship between variables. This can result in predictions that are way off.\n",
    "##### When independent variables in a regression model are strongly related to each other, it becomes hard to determine the true effect of each variable on the outcome.\n",
    "##### Linear regression is used for continuous target variables and is not suitable for classification problems (predicting categories). While logistic regression exists, it's a classification algorithm despite the name.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6359be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8183205",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94857af8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
